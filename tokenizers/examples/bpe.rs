use tokenizers::models::bpe::{BpeTrainerBuilder, BPE};
use tokenizers::normalizers::{strip::Strip, utils::Sequence};
use tokenizers::pre_tokenizers::byte_level::ByteLevel;
use tokenizers::{AddedToken, Result, TokenizerBuilder};

fn main() -> Result<()> {
    let initial_vocab: [&str; 257] = [
        "<|endoftext|>",
        "!",
        "\"",
        "#",
        "$",
        "%",
        "&",
        "'",
        "(",
        ")",
        "*",
        "+",
        ",",
        "-",
        ".",
        "/",
        "0",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        ":",
        ";",
        "<",
        "=",
        ">",
        "?",
        "@",
        "A",
        "B",
        "C",
        "D",
        "E",
        "F",
        "G",
        "H",
        "I",
        "J",
        "K",
        "L",
        "M",
        "N",
        "O",
        "P",
        "Q",
        "R",
        "S",
        "T",
        "U",
        "V",
        "W",
        "X",
        "Y",
        "Z",
        "[",
        "\\",
        "]",
        "^",
        "_",
        "`",
        "a",
        "b",
        "c",
        "d",
        "e",
        "f",
        "g",
        "h",
        "i",
        "j",
        "k",
        "l",
        "m",
        "n",
        "o",
        "p",
        "q",
        "r",
        "s",
        "t",
        "u",
        "v",
        "w",
        "x",
        "y",
        "z",
        "{",
        "|",
        "}",
        "~",
        "¡",
        "¢",
        "£",
        "¤",
        "¥",
        "¦",
        "§",
        "¨",
        "©",
        "ª",
        "«",
        "¬",
        "®",
        "¯",
        "°",
        "±",
        "²",
        "³",
        "´",
        "µ",
        "¶",
        "·",
        "¸",
        "¹",
        "º",
        "»",
        "¼",
        "½",
        "¾",
        "¿",
        "À",
        "Á",
        "Â",
        "Ã",
        "Ä",
        "Å",
        "Æ",
        "Ç",
        "È",
        "É",
        "Ê",
        "Ë",
        "Ì",
        "Í",
        "Î",
        "Ï",
        "Ð",
        "Ñ",
        "Ò",
        "Ó",
        "Ô",
        "Õ",
        "Ö",
        "×",
        "Ø",
        "Ù",
        "Ú",
        "Û",
        "Ü",
        "Ý",
        "Þ",
        "ß",
        "à",
        "á",
        "â",
        "ã",
        "ä",
        "å",
        "æ",
        "ç",
        "è",
        "é",
        "ê",
        "ë",
        "ì",
        "í",
        "î",
        "ï",
        "ð",
        "ñ",
        "ò",
        "ó",
        "ô",
        "õ",
        "ö",
        "÷",
        "ø",
        "ù",
        "ú",
        "û",
        "ü",
        "ý",
        "þ",
        "ÿ",
        "Ā",
        "ā",
        "Ă",
        "ă",
        "Ą",
        "ą",
        "Ć",
        "ć",
        "Ĉ",
        "ĉ",
        "Ċ",
        "ċ",
        "Č",
        "č",
        "Ď",
        "ď",
        "Đ",
        "đ",
        "Ē",
        "ē",
        "Ĕ",
        "ĕ",
        "Ė",
        "ė",
        "Ę",
        "ę",
        "Ě",
        "ě",
        "Ĝ",
        "ĝ",
        "Ğ",
        "ğ",
        "Ġ",
        "ġ",
        "Ģ",
        "ģ",
        "Ĥ",
        "ĥ",
        "Ħ",
        "ħ",
        "Ĩ",
        "ĩ",
        "Ī",
        "ī",
        "Ĭ",
        "ĭ",
        "Į",
        "į",
        "İ",
        "ı",
        "Ĳ",
        "ĳ",
        "Ĵ",
        "ĵ",
        "Ķ",
        "ķ",
        "ĸ",
        "Ĺ",
        "ĺ",
        "Ļ",
        "ļ",
        "Ľ",
        "ľ",
        "Ŀ",
        "ŀ",
        "Ł",
        "ł",
        "Ń",
    ];

    // cargo run --example bpe

    let vocab_size: usize = 50000 + 257;

    let initial_tokens = initial_vocab
        .map(|x| AddedToken::from(String::from(x), true))
        .to_vec();

    let mut trainer = BpeTrainerBuilder::new()
        .show_progress(true)
        .vocab_size(vocab_size)
        .min_frequency(0)
        .special_tokens(initial_tokens)
        .build();

    let mut tokenizer = TokenizerBuilder::new()
        .with_model(BPE::default())
        .with_normalizer(Some(Sequence::new(vec![Strip::new(false, false).into()])))
        .with_pre_tokenizer(Some(ByteLevel::default()))
        .with_post_processor(Some(ByteLevel::default()))
        .with_decoder(Some(ByteLevel::default()))
        .build()?;

    let pretty = true;
    tokenizer
        .train_from_files(
            &mut trainer,
            vec!["../../../embedding-explorer/tokenizers/wikitext.txt".to_string()],
        )?
        .save("tokenizer.json", pretty)?;

    Ok(())
}
